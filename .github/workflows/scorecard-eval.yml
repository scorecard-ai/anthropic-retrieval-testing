name: Anthropic RAG Eval Workflow

on: 
  # push:
  #   branches:
  #     - main
  # pull_request:
  #   branches:
  #     - main
  workflow_dispatch:
    inputs:
      input_testset_id:
        description: 'Testset ID'
        required: true
      scoring_config_id:
        description: 'Scoring Config ID'
        required: true
  repository_dispatch:
    types: start-evaluation
      
permissions:
  contents: read

jobs:
  evaluation-test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set PR testset and scoring config
      if: github.event_name == 'push' || github.event_name == 'pull_request'
      run: |
        echo "DEFAULT_TESTSET_ID=86" >> $GITHUB_ENV
        echo "DEFAULT_SCORING_CONFIG_ID=1" >> $GITHUB_ENV

    - name: Run test_wiki_e2e
      env:
        # API keys
        SCORECARD_API_KEY: ${{ secrets.SCORECARD_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        
        # Testset and Scoring Config values
        # 1. Check if there's an input from manual trigger (workflow_dispatch)
        # 2. Fallback to values sent from external sources (repository_dispatch)
        # 3. Use default values set as environment variables if neither is available
        INPUT_TESTSET_ID: ${{ github.event.inputs.input_testset_id || github.event.client_payload.input_testset_id || env.DEFAULT_TESTSET_ID }}
        SCORING_CONFIG_ID: ${{ github.event.inputs.scoring_config_id || github.event.client_payload.scoring_config_id || env.DEFAULT_SCORING_CONFIG_ID }}
      run: python3 tests/test_wiki_e2e.py
